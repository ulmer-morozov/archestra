import { useChat } from '@ai-sdk/react';
import { DefaultChatTransport } from 'ai';
import { useEffect, useMemo, useState } from 'react';

import { Skeleton } from '@ui/components/ui/skeleton';
import { useChatStore } from '@ui/stores/chat-store';
import { useCloudProvidersStore } from '@ui/stores/cloud-providers-store';
import { useOllamaStore } from '@ui/stores/ollama-store';

import ChatHistory from './ChatHistory';
import ChatInput from './ChatInput';
import SystemPrompt from './SystemPrompt';

interface ChatPageProps {}

export default function ChatPage(_props: ChatPageProps) {
  const { getCurrentChat } = useChatStore();
  const { selectedModel } = useOllamaStore();
  const { availableCloudProviderModels } = useCloudProvidersStore();
  const [localInput, setLocalInput] = useState('');

  const currentChat = getCurrentChat();
  const currentChatSessionId = currentChat?.sessionId || '';
  const currentChatMessages = currentChat?.messages || [];

  // Create transport that changes based on model selection
  const transport = useMemo(() => {
    // Check if it's a cloud model
    const isCloudModel = availableCloudProviderModels.some((m) => m.id === selectedModel);

    // Use OpenAI endpoint for cloud models, Ollama for local
    const apiEndpoint = isCloudModel ? '/api/llm/openai/stream' : '/api/llm/ollama/stream';

    return new DefaultChatTransport({
      api: apiEndpoint,
      body: {
        model: selectedModel || 'llama3.1:8b',
        sessionId: currentChatSessionId,
      },
      fetch: async (input, init) => {
        // Override fetch to use the correct backend URL
        const url = typeof input === 'string' ? input : input.url;
        const fullUrl = url.startsWith('http') ? url : `http://localhost:3456${url}`;
        return fetch(fullUrl, init);
      },
    });
  }, [selectedModel, currentChatSessionId, availableCloudProviderModels]);

  const { sendMessage, messages, setMessages, stop, status, error } = useChat({
    id: currentChatSessionId || 'temp-id', // use the provided chat ID or a temp ID
    transport,
    /**
     * TODO: we probably need to map our messages to what the ai-sdk expects here
     */
    initialMessages: currentChatMessages,
    onFinish: (message) => {
      console.log('Message finished:', message);
    },
    onError: (error) => {
      console.error('Chat error:', error);
    },
    setMessages: (messages) => {
      console.log('Setting messages:', messages);
      setMessages(messages);
    },
  });

  const isLoading = status === 'streaming';

  // Log messages updates
  useEffect(() => {
    console.log('All messages in ChatPage:', messages);
    console.log('Messages length:', messages.length);
    console.log('isLoading:', isLoading);
    console.log('error:', error);
    const lastMessage = messages[messages.length - 1];
    if (lastMessage) {
      console.log('Last message:', lastMessage);
    }
  }, [messages, isLoading, error]);

  const handleInputChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    setLocalInput(e.target.value);
  };

  const handleSubmit = (e?: React.FormEvent<HTMLFormElement>) => {
    e?.preventDefault();
    if (localInput.trim()) {
      console.log('Sending message:', localInput);
      sendMessage({ text: localInput });
      setLocalInput('');
    }
  };

  // Early return after all hooks have been called
  if (!currentChat) {
    return <Skeleton className="h-full w-full" />;
  }

  return (
    <div className="flex flex-col h-full gap-2 max-w-full overflow-hidden">
      <div className="flex-1 min-h-0 overflow-hidden max-w-full">
        <ChatHistory messages={messages} />
      </div>
      <SystemPrompt />
      <div className="flex-shrink-0">
        <ChatInput
          input={localInput}
          handleInputChange={handleInputChange}
          handleSubmit={handleSubmit}
          isLoading={isLoading}
          stop={stop}
        />
      </div>
    </div>
  );
}
